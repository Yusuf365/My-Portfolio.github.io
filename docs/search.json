[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Mohammmed Yusuf Shaikh",
    "section": "",
    "text": "Hello there! I’m Mohammed Yusuf Shaikh. A tech enthusiast, with ever-curious mind. I spend my days exploring the vast landscape of data analysis & software development. With my keen interest in Data Science.\n\n\nMy professional journey has been marked by numerous successful academic projects where data was at the forefront. I’ve had the privilege of working across various industry verticals, from finance to IT, providing data visualization, predictive modeling, and strategic analysis.\n\n\n\nTools: Github, MS Excel, MS Word, MS PowerPoint, R, STATA, Python, Tableau, Power BI, MySQL, PostgreSQL\n\n\n\n\n\nCreative Problem Solving\nTeam Collaboration\nCommunication\nAdaptability\nCritical Thinking\n\n\n\n\n\nI’m always open to discussing new projects or opportunities. Reach out to me on Linkedin or via Email.\nHome | Projects | Resume"
  },
  {
    "objectID": "about.html#professional-journey",
    "href": "about.html#professional-journey",
    "title": "Mohammmed Yusuf Shaikh",
    "section": "",
    "text": "My professional journey has been marked by numerous successful academic projects where data was at the forefront. I’ve had the privilege of working across various industry verticals, from finance to IT, providing data visualization, predictive modeling, and strategic analysis.\n\n\n\nTools: Github, MS Excel, MS Word, MS PowerPoint, R, STATA, Python, Tableau, Power BI, MySQL, PostgreSQL\n\n\n\n\n\nCreative Problem Solving\nTeam Collaboration\nCommunication\nAdaptability\nCritical Thinking"
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "Mohammmed Yusuf Shaikh",
    "section": "",
    "text": "I’m always open to discussing new projects or opportunities. Reach out to me on Linkedin or via Email.\nHome | Projects | Resume"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects Portfolio",
    "section": "",
    "text": "Here you can see a selection of my latest projects.\n\n\n\nSummary: Delve into Toronto’s crime landscape from 2014 to 2022 with this analytical exploration based on the Toronto Police Annual Statistical Report. Uncover trends, patterns, and demographic insights using R language, offering valuable perspectives to shape crime prevention policies and strategies for enhancing public safety\nTools: R language\nOutcome: Contributed to a better understanding of demographic impacts on crime rates.\nGitHub Repository: View Code\nRead the Analysis: Read paper\n\n\n\n\n\nSummary: The paper aims at reproducing the SPCA results to show clearer interpretability than standard PCA. Using R’s elasticnet package, the study reformulates PCA as an elastic-net problem, follows the original simulation setup, and compares PCA to SPCA with two components constrained to four nonzeros each. SPCA recovers the intended factor structure, matches PCA on variance explained within tolerance, and delivers sparser, more interpretable loadings, with reproducible code.\nTools: R Code\nGitHub Repository: View Code\nRead the Analysis: Read paper\nPresentation: Presentation PDF\n\n\n\n\n\nSummary: This project showcases an end-to-end data analysis workflow using R, using tidyverse, dplyr, readr, ggplot2, janitor, broom, gtsummary, MASS, AER, reshape2, and knitr to clean and validate raw data, perform focused EDA, and build interpretable models across two distinct & separate datasets: airline customer satisfaction and YouTube revenue. A binomial logistic regression quantified how operational reliability and core service ratings drive satisfaction levels. On the other end, a log-linear multiple regression showed that ad impressions are the dominant determinant of revenue, with playlist views and video duration adding smaller gains. Model selection relied on AIC and likelihood-ratio tests, and diagnostics (residual and QQ plots, multicollinearity checks) supported the specifications.\nTools: R language\nGitHub Repository: View Code\nRead the Analysis: Read paper\n\n\n\n\n\nSummary: The project introduces Lévy processes—random motions with independent, stationary increments that include Brownian motion and Poisson jumps. Using the Lévy–Khintchine formula, the process splits movement into drift, diffusion, and jumps, allowing it to model heavy tails and capture abrupt shocks that the basic Black–Scholes model misses. In practice, jumps improve option pricing and tail-risk measures by capturing skewness and heavy tails. But markets also show volatility clustering and dependence, so a bare Lévy model falls short; add time changes or stochastic volatility and test out-of-sample. Bottom line: use a Lévy core for realism, then layer smarter dynamics for precision.\nTools: R Markdown\nGitHub Repository: View Code\nRead the Analysis: Read paper\n\n\n\n\n\nSummary: Conducted an econometric analysis examining the impact of minimum wage changes on unemployment rates post-financial crisis in Ontario, Quebec, and British Columbia. The analysis leverages STATA and Python for data processing and statistical modeling, facilitating a comprehensive examination of regional economic trends and policy impacts. The study utilized regression models to assess the relationship between minimum wage policies and unemployment trends, providing valuable insights into labor market dynamics within the selected provinces.\nTools: STATA, Python\nOutcome: Offered policy insights on labor market dynamics.\nGithub Repository: View Code\nRead the Analysis: Read Econometric Analysis\n\n\n\n\n\nSummary: Evaluate the economic landscape of the Miami Heat basketball team during the 2021-22 NBA season through a meticulous analysis conducted using Excel. Delve into player salaries and Marginal Revenue Product (MRP) evaluation, unveiling insights into player valuation and team financial dynamics within the professional sports arena.\nTools: MS Excel, MS Word\nOutcome: Assisted in assessing player value and team economics.\nGitHub Repository: View Files\nRead the Analysis: Read Paper\n\n\n\n\n\nSummary: Explore the intricate relationship between market concentration, passenger volume, and airfare dynamics in the U.S. domestic airline industry during the late 1990s and early 2000s. Leveraging panel data sourced from the U.S. Department of Transportation, this econometric analysis investigates the factors influencing average one-way airfare across 1149 domestic flight routes over four years. STATA is utilized as the primary tool for data analysis, enabling a comprehensive exploration of factors shaping airfare trends in this pivotal era of the aviation industry.\nTools: STATA, R Language, MS Word, MS PowerPoint\nOutcome: Enhanced understanding of aviation industry pricing strategies.\nGitHub Repository: View Code\nRead the Analysis: Read Paper\n\n\n\n\n\nSummary: This project employs R language for data analysis to examine the relationship between academic advising and student success at the University of Toronto. Despite a limited sample size, the study utilizes statistical techniques such as hypothesis testing, linear regression, and factor analysis to explore correlations between advising utilization, communication effectiveness, and timely graduation. The findings aim to offer insights into the impact of academic advising on student outcomes, informing potential strategies for improving academic support services.\nTools: R language, MS Word, MS PowerPoint\nOutcome: Informed potential improvements for academic support services.\nGitHub Repository: View Code\nRead the Analysis: Read Report\n\nHome | About | Resume"
  },
  {
    "objectID": "projects.html#featured-projects",
    "href": "projects.html#featured-projects",
    "title": "Projects Portfolio",
    "section": "",
    "text": "Here you can see a selection of my latest projects.\n\n\n\nSummary: Delve into Toronto’s crime landscape from 2014 to 2022 with this analytical exploration based on the Toronto Police Annual Statistical Report. Uncover trends, patterns, and demographic insights using R language, offering valuable perspectives to shape crime prevention policies and strategies for enhancing public safety\nTools: R language\nOutcome: Contributed to a better understanding of demographic impacts on crime rates.\nGitHub Repository: View Code\nRead the Analysis: Read paper\n\n\n\n\n\nSummary: The paper aims at reproducing the SPCA results to show clearer interpretability than standard PCA. Using R’s elasticnet package, the study reformulates PCA as an elastic-net problem, follows the original simulation setup, and compares PCA to SPCA with two components constrained to four nonzeros each. SPCA recovers the intended factor structure, matches PCA on variance explained within tolerance, and delivers sparser, more interpretable loadings, with reproducible code.\nTools: R Code\nGitHub Repository: View Code\nRead the Analysis: Read paper\nPresentation: Presentation PDF\n\n\n\n\n\nSummary: This project showcases an end-to-end data analysis workflow using R, using tidyverse, dplyr, readr, ggplot2, janitor, broom, gtsummary, MASS, AER, reshape2, and knitr to clean and validate raw data, perform focused EDA, and build interpretable models across two distinct & separate datasets: airline customer satisfaction and YouTube revenue. A binomial logistic regression quantified how operational reliability and core service ratings drive satisfaction levels. On the other end, a log-linear multiple regression showed that ad impressions are the dominant determinant of revenue, with playlist views and video duration adding smaller gains. Model selection relied on AIC and likelihood-ratio tests, and diagnostics (residual and QQ plots, multicollinearity checks) supported the specifications.\nTools: R language\nGitHub Repository: View Code\nRead the Analysis: Read paper\n\n\n\n\n\nSummary: The project introduces Lévy processes—random motions with independent, stationary increments that include Brownian motion and Poisson jumps. Using the Lévy–Khintchine formula, the process splits movement into drift, diffusion, and jumps, allowing it to model heavy tails and capture abrupt shocks that the basic Black–Scholes model misses. In practice, jumps improve option pricing and tail-risk measures by capturing skewness and heavy tails. But markets also show volatility clustering and dependence, so a bare Lévy model falls short; add time changes or stochastic volatility and test out-of-sample. Bottom line: use a Lévy core for realism, then layer smarter dynamics for precision.\nTools: R Markdown\nGitHub Repository: View Code\nRead the Analysis: Read paper\n\n\n\n\n\nSummary: Conducted an econometric analysis examining the impact of minimum wage changes on unemployment rates post-financial crisis in Ontario, Quebec, and British Columbia. The analysis leverages STATA and Python for data processing and statistical modeling, facilitating a comprehensive examination of regional economic trends and policy impacts. The study utilized regression models to assess the relationship between minimum wage policies and unemployment trends, providing valuable insights into labor market dynamics within the selected provinces.\nTools: STATA, Python\nOutcome: Offered policy insights on labor market dynamics.\nGithub Repository: View Code\nRead the Analysis: Read Econometric Analysis\n\n\n\n\n\nSummary: Evaluate the economic landscape of the Miami Heat basketball team during the 2021-22 NBA season through a meticulous analysis conducted using Excel. Delve into player salaries and Marginal Revenue Product (MRP) evaluation, unveiling insights into player valuation and team financial dynamics within the professional sports arena.\nTools: MS Excel, MS Word\nOutcome: Assisted in assessing player value and team economics.\nGitHub Repository: View Files\nRead the Analysis: Read Paper\n\n\n\n\n\nSummary: Explore the intricate relationship between market concentration, passenger volume, and airfare dynamics in the U.S. domestic airline industry during the late 1990s and early 2000s. Leveraging panel data sourced from the U.S. Department of Transportation, this econometric analysis investigates the factors influencing average one-way airfare across 1149 domestic flight routes over four years. STATA is utilized as the primary tool for data analysis, enabling a comprehensive exploration of factors shaping airfare trends in this pivotal era of the aviation industry.\nTools: STATA, R Language, MS Word, MS PowerPoint\nOutcome: Enhanced understanding of aviation industry pricing strategies.\nGitHub Repository: View Code\nRead the Analysis: Read Paper\n\n\n\n\n\nSummary: This project employs R language for data analysis to examine the relationship between academic advising and student success at the University of Toronto. Despite a limited sample size, the study utilizes statistical techniques such as hypothesis testing, linear regression, and factor analysis to explore correlations between advising utilization, communication effectiveness, and timely graduation. The findings aim to offer insights into the impact of academic advising on student outcomes, informing potential strategies for improving academic support services.\nTools: R language, MS Word, MS PowerPoint\nOutcome: Informed potential improvements for academic support services.\nGitHub Repository: View Code\nRead the Analysis: Read Report\n\nHome | About | Resume"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mohammed Yusuf Shaikh",
    "section": "",
    "text": "Hello, my name is Mohammed Yusuf Shaikh, and I recently graduated in June 2025 with a Honours Bachelor of Science in Economics and Applied Statistics from the University of Toronto. I am deeply passionate about data-driven decision-making, Smart Applications improving Citizen Services including Smart governance, Data Analytics, financial markets. Furthermore, my hands-on experience with MS Excel and Python has equipped me with the necessary skills for data research and analysis skills. I’m proficient in MS Excel, MS PowerPoint, R, STATA, SQL, and Tableau.\nRecently, I completed London School of Economics, Summer School course Machine Learning and Stocashtic Simulation, ME319 during from 4 August to 22 August 2025 in London, UK. The module covered Monte Carlo methods, stochastic process simulation, GANs for risk, linking GLMs to deep neural networks, and rigorous model calibration, with hands-on Python case studies across finance, insurance, and risk management.\nDelve into my latest projects, where I apply my unique blend of skills and perspectives to create innovative solutions. From web applications that streamline processes to software tools that solve complex problems, each project reflects my commitment to pushing boundaries and making a tangible impact.\n\nLet’s Connect!\nI’m always open to new opportunities, collaborations, or just a friendly chat. Feel free to connect with me via email or Linkedin. You can also download my resume to learn more about my experience."
  },
  {
    "objectID": "Project 6/Data Analysis_v1.html#airline-dataset",
    "href": "Project 6/Data Analysis_v1.html#airline-dataset",
    "title": "Regression Analysis: Technical Report",
    "section": "2.1 Airline Dataset",
    "text": "2.1 Airline Dataset\n\n2.1.1 Dataset Source\nThe data used in this paper were collected from Kaggle website Library. The specific dataset used in this paper is the ‘Airline Customer satisfaction’ (Osuyah 2023). The dataset is categorized by collecting feedback from customer after they have traveled with the airline and determining if customer was satisfied or dissatisfied by the services. The actual name of the airline is not provided in the data due privacy concerns so the alias name used is Invistico Airlines.\nKey variables\nThe dataset provides a comprehensive overview of various factors influencing customer satisfaction in airline services. It includes a variety of metrics categorized into key domains, such as demographic details, travel characteristics, service quality ratings, and performance metrics. We used many variables such as Demographic Information: gender, passenger age, Travel Characteristics: class: (Economy, Business, or Economy Plus), flight distance, Service Quality Metrics (Rated on a scale of 0–5): seat_comfort, food and drink, gate location, inflight entertainment, ease of online booking, cleanliness and other services like leg room, check-in service. One other parameter was departure delay in minutes that is delay in flight departure time"
  },
  {
    "objectID": "Project 6/Data Analysis_v1.html#youtube-dataset",
    "href": "Project 6/Data Analysis_v1.html#youtube-dataset",
    "title": "Regression Analysis: Technical Report",
    "section": "2.2 Youtube Dataset",
    "text": "2.2 Youtube Dataset\n\n2.2.1 Dataset Source\nThe data used in this paper were collected from Kaggle website Library. The dataset used in this paper is the ‘Youtube Channel Real Performance Analytics’ (Alexey 2023). The YouTube video analytics, focuses on key performance, engagement, revenue, and audience metrics. It enables insights into video duration, upload timing, and ad impressions’ impact on monetization and audience retention.\nKey variables\nThis dataset provides comprehensive information about video performance, revenue generation, and audience engagement, which can be used for detailed analytics and insights and paper focuses on predicting the estimated revenue (USD) as the dependent variable and three independent variables, the number of views on the playlist associated with the videos (playlist_views), the number of ad impressions generated by the videos (ad_impressions), and the duration of the video content (video_duration).\nData used in this paper was downloaded, cleaned and analyzed with the programming language R (R Core Team 2022). Also the paper used with support of additional packages in R: ‘tidyverse’ (Wickham et al. 2019), ‘janitor’ (Firke 2021), ‘dplyr’ (Wickham et al. 2022), ‘readr’ (Wickham, Hester, and Bryan 2024), ‘ggplot’ (Wickham 2016), ‘AER’(Kleiber and Zeileis 2008), ‘gtsummary’ (Sjoberg et al. 2021), ‘reshape2’ (Wickham 2007), ‘broom’ (Robinson, Hayes, and Couch 2024), ‘MASS’ (Venables and Ripley 2002)."
  },
  {
    "objectID": "Project 6/Data Analysis_v1.html#binomial-logisitic-regression",
    "href": "Project 6/Data Analysis_v1.html#binomial-logisitic-regression",
    "title": "Regression Analysis: Technical Report",
    "section": "3.1 Binomial Logisitic Regression",
    "text": "3.1 Binomial Logisitic Regression\nThe Invisco airline collected the feedback from customers. The model aims to find the probability of the binary outcome i.e. passenger satisfaction:\n\n( y = 0 ): Customer is dissatisfied.\n( y = 1 ): Customer is satisfied.\n\nThe binomial logistic regression model predicts the likelihood of being satisfied (P(Y = 1)) based on independent variables. The probability of a binary outcome Y being 1 is modeled as:\n\\[\nP(Y = 1) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p}}\n\\]\nOr equivalently, the log-odds form is given by:\n\\[\n\\log\\left(\\frac{P(Y=1)}{1 - P(Y=1)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\nWhere:\n\n\\(P(Y = 1)\\): Probability that the customer is satisfied.\n\\(\\frac{P(Y=1)}{1 - P(Y=1)}\\): Odds of satisfaction.\n\\(\\beta_0\\): Intercept of the model.\n\\(\\beta_1, \\beta_2, \\dots, \\beta_p\\): Coefficients representing the effect of each predictor"
  },
  {
    "objectID": "Project 6/Data Analysis_v1.html#multiple-linear-regression",
    "href": "Project 6/Data Analysis_v1.html#multiple-linear-regression",
    "title": "Regression Analysis: Technical Report",
    "section": "3.2 Multiple Linear Regression",
    "text": "3.2 Multiple Linear Regression\nUsing a Multiple Linear Regression (MLR) model to predict the dependent variable Estimated Revenue (USD) based on a set of independent variables from the YouTube dataset. The goal is to understand how key performance metrics influence revenue generation and provide actionable insights for optimization.\nThe general equation for Multiple Linear Regression is: \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon\n\\]\nFor this analysis, the log-transformed equation used:\n\\[\n\\log(\\text{Estimated Revenue (USD)}) = \\beta_0 + \\beta_1 (\\text{Playlist Views}) + \\beta_2 (\\text{Ad Impressions}) + \\beta_3 (\\text{Video Duration}) + \\epsilon\n\\]"
  },
  {
    "objectID": "Project 6/Data Analysis_v1.html#binomial-logisitic-regression-1",
    "href": "Project 6/Data Analysis_v1.html#binomial-logisitic-regression-1",
    "title": "Regression Analysis: Technical Report",
    "section": "4.1 Binomial Logisitic Regression",
    "text": "4.1 Binomial Logisitic Regression\nThe correlation matrix from Figure 1 highlights strong positive correlation between arrival delays and departure delays of 0.97, indicating that delays at departure often propagate to arrival times. Additionally, service metrics such as ease of online booking and online boarding are moderately correlated (0.68), reflecting their positive relationship with customer experience. Service-related variables like inflight entertainment and food and drink also show moderate positive associations (~0.43), suggesting that satisfaction in one area influences the other. This analysis underscores the need to address extreme delays while enhancing clustered service areas for better customer satisfaction.\n\n\n\n\n\n\n\n\nFigure 1: Heat Map or corr matrix\n\n\n\n\n\nThe Table 1 shows the result of binomial logistic regression analysis. The models uses multiple predictors. The predictors include numerical predictors and categorical predictors, so we use Gender and female as the reference level and for Business class as the reference level. In the Table 1 we observe, male passenger report higher satisfaction than females, indicating they are more than twice (2.43) likely to be satisfied. Similarly, customers in Eco and Eco Plus classes have a higher relative satisfaction likelihood after adjusting for expectations and service quality using business class as the reference level. However, services offered by the airline such as inflight entertainment, seat comfort, cleanliness, food & drink and ease of online booking negatively correlated by customer satisfaction. The Inflight entertainment is one of the strongest negative predictors of customer satisfaction as Inflight Entertainment (-0.8165) the largest negative log-odds, indicating that poor ratings in entertainment strongly reduces satisfaction.\n\n\n\n\nTable 1: Model 1\n\n\n\n\nBinomial Logistic Regression Results\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nodds_ratio\n\n\n\n\n(Intercept)\n5.3191244\n0.0559170\n95.125310\n0\n204.2050029\n\n\nflight_distance\n0.0000719\n0.0000081\n8.855758\n0\n1.0000719\n\n\ndeparture_delay_in_minutes\n0.0046094\n0.0002130\n21.643370\n0\n1.0046200\n\n\ncleanliness\n-0.1034490\n0.0075648\n-13.675118\n0\n0.9017220\n\n\ninflight_entertainment\n-0.8164875\n0.0074769\n-109.202016\n0\n0.4419814\n\n\nseat_comfort\n-0.2606842\n0.0087304\n-29.859229\n0\n0.7705242\n\n\nage\n-0.0043520\n0.0005263\n-8.268296\n0\n0.9956575\n\n\ngenderMale\n0.8899748\n0.0155075\n57.389899\n0\n2.4350682\n\n\nclassEco\n1.1505727\n0.0166927\n68.926771\n0\n3.1600022\n\n\nclassEco Plus\n0.9922172\n0.0302824\n32.765479\n0\n2.6972081\n\n\nfood_and_drink\n0.3244823\n0.0087978\n36.882340\n0\n1.3833143\n\n\nleg_room_service\n-0.2757300\n0.0065272\n-42.243466\n0\n0.7590178\n\n\ngate_location\n-0.0576163\n0.0069299\n-8.314168\n0\n0.9440121\n\n\nease_of_online_booking\n-0.4804120\n0.0068167\n-70.475289\n0\n0.6185285\n\n\ncheckin_service\n-0.2724898\n0.0062839\n-43.363294\n0\n0.7614812\n\n\n\n\n\n\n\n\nMoving from logistic regression we perform model comparison to ensure both model are accurate. A full model includes chosen predictors whereas the reduced model includes a subset of predictor use in full model as seen in Table 1. The reduced logistic regression model in Table 2 shows the relationship between customer satisfaction and a selected set of predictors. The compare the models we use AIC as statistical measure used to evaluate and compare the quality of models.\n\n\n\n\nTable 2: Model 2\n\n\n\n\nReduced Binomial Logistic Regression Model Results\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nStd. Error\nz-value\np-value\nConf. Low\nConf. High\n\n\n\n\n(Intercept)\n3.7406\n0.0417\n89.6859\n0\n3.6590\n3.8225\n\n\ndeparture_delay_in_minutes\n0.0041\n0.0002\n20.3415\n0\n0.0037\n0.0045\n\n\ncleanliness\n-0.4586\n0.0064\n-71.8510\n0\n-0.4711\n-0.4461\n\n\ninflight_entertainment\n-0.9084\n0.0069\n-132.5327\n0\n-0.9219\n-0.8950\n\n\nseat_comfort\n-0.3667\n0.0081\n-45.3466\n0\n-0.3826\n-0.3509\n\n\nage\n-0.0088\n0.0005\n-18.3363\n0\n-0.0097\n-0.0078\n\n\ngenderMale\n0.9216\n0.0146\n63.1258\n0\n0.8930\n0.9502\n\n\nclassEco\n1.1685\n0.0155\n75.1639\n0\n1.1381\n1.1990\n\n\nclassEco Plus\n1.0175\n0.0286\n35.5922\n0\n0.9615\n1.0736\n\n\nfood_and_drink\n0.3745\n0.0076\n49.2733\n0\n0.3597\n0.3895\n\n\n\n\n\n\n\n\nIt helps to identify the model that best balances goodness-of-fit and simplicity. From Table 3 we obverse AIC for 2 models. The lower the AIC better the model, therefore, Table 1 AIC equivalent to 108635.4 results are better compared to reduced model AIC. Table 3 shows the full model explains customer satisfaction substantially better than the reduced model, as evidenced by the large reduction in deviance that indicates additional predictors in the full model provide a significantly better fit to the data. We observe p-value is extremely small so we reject the null hypothesis.\n\n\n\n\nTable 3: Analysis of Deviance Table: Comparison of Reduced and Full Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nResidual Df\nResidual Deviance\nDf\nDeviance\nPr(&gt;Chi)\n\n\n\n\nReduced Model\n129477\n118909.3\nNA\nNA\nNA\n\n\nFull Model\n129472\n108605.4\n5\n10303.97\n0"
  },
  {
    "objectID": "Project 6/Data Analysis_v1.html#multiple-linear-regression-1",
    "href": "Project 6/Data Analysis_v1.html#multiple-linear-regression-1",
    "title": "Regression Analysis: Technical Report",
    "section": "4.2 Multiple Linear Regression",
    "text": "4.2 Multiple Linear Regression\nFor the youtube datatset which analysis evaluates the performance of a regression model. We compare multiple linear regression with a log transformed regression model. From Table 4 the model evaluates estimated revenue generated in USD by YouTube videos. The intercept (0.514) is statistically significant, indicating when all predictors are zero log-transformed revenue is approximately 0.5134 units. All predictors are statistically significant and they have all have positive relationship with the dependent variables. The p-value is less than 0.05, indicating strong evidence. Ad impressions are the most impact predictor, as 1-unit increase in ad impressions leads to a 0.003% increase in revenue.\n\n\n\n\nTable 4: Regression Analysis of Log-Transformed Estimated Revenue\n\n\n\n\n\n\nTerm\nEstimate\nStd. Error\nt-value\np-value\n\n\n\n\n(Intercept)\n0.51378\n0.09705\n5.29412\n0.00000\n\n\nplaylist_views\n0.00026\n0.00008\n3.42790\n0.00068\n\n\nad_impressions\n0.00003\n0.00000\n20.09616\n0.00000\n\n\nvideo_duration\n0.00070\n0.00012\n5.61654\n0.00000\n\n\n\n\n\n\n\n\nFigure 2 shows QQ-Plot follows a theoretical distribution indicating the residuals are normally distributed which help to check the linearity assumptions. As the residuals re distributed around red line with some curvature suggesting slight deviation. We can concludethe residuals are near normal.\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nIn Figure 3 we plot the relationship between residuals and predicted values for a log transformed regression. The The residuals are centered around the red horizontal line at y=0, indicating that the model does not exhibit systematic bias in predictions (e.g., overestimating or underestimating consistently). Ideally, the residuals should have a constant spread (homoscedasticity). There appears to be heteroscedasticity, as the spread of residuals increases with larger fitted values. This suggests that the variance of residuals is not constant and increases as fitted values grow. A non-linear relationship between the predictors and the response variable that is not captured by the model.\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nWe considered many models including by introducing the polynomial or interaction terms in the model to capture non-linear relationships ,however, we did not see an improvement. We added complexity but it did not translate into better predictive power, and the model’s assumptions were still not fully satisfied. Adding more complexity to the model may lead to over fitting, especially if the added terms are not improving the model’s predictive accuracy."
  },
  {
    "objectID": "Project 7 ML/Simulation spca STA315 Project.html",
    "href": "Project 7 ML/Simulation spca STA315 Project.html",
    "title": "1 Introduction",
    "section": "",
    "text": "Principal Component Analysis (PCA) a method we know is used as statistical technique to reduce the dimensionality of data. PCA Methods strategically aims to find a “good” linear combination of \\(X1,.....,X_p\\) such that the linear combination explains the most variability or randomness present in the dataset.\nMathematically, each \\(i\\)-th principal component is a linear combination of the original features:\n\\[\nY_i = \\boldsymbol{\\ell}_i^\\top \\mathbf{X}\n\\]\nWhere,\n\n\\(Y_i \\text{ is the } i\\text{-th principal component}\\)\n\\(\\mathbf{X} = (X_1, X_2, \\ldots, X_p)^\\top \\text{ is the vector of original features}\\)\n\\(\\boldsymbol{\\ell}_i = (\\ell_{i1}, \\ell_{i2}, \\ldots, \\ell_{ip})^\\top \\text{ is the loading vector (weights)}\\)\n\nThe paper discusses that PCA method uses Singular Value Decomposition (SVD) to compute principal components, where X is a standardized data matrix. The X data matrix can be broken down to \\(Z=UD\\) which represnet principal component and matrix \\(V\\) are the loadings that explain how much each original variable contributes to each principal component.\n\\[ X = UDV^T\\]\n\n\\(U \\text{ contains the Eigenvectors }\\)\n\\(D \\text{ contains the Eigenvalues }\\)\n\nAlthough in PCA method the components capture maximum variance and compresses data while retaining important information despite that we have a larger drawback that is PCA uses all the variables when creating each principal components as a result the PC ends up being a mixture of every variable making it harder to interpret therefore lack of sparsity implies PCA doesn’t do variable selection. Moreover, the loadings are usually nonzero, making it hard to interpret which variables that are truly important and contribute to principal component.\nTo overcome the limitation of the PCA, a paper was presented by Hui Zou, Trevor Hastie, and Robert Tibshirani(2006). The paper (Zou, Hastie, and Tibshirani 2006) introduces a new method called Sparse Principal Component Analysis (SPCA). The idea focused on to make PCA a more interpretable method by forcing some of the loadings to be exactly zero. This paper presents a detailed review of SPCA, explains its methodology, compares it with existing alternatives, and reproduces a key simulation study from the original paper."
  },
  {
    "objectID": "Project 7 ML/Simulation spca STA315 Project.html#generating-data",
    "href": "Project 7 ML/Simulation spca STA315 Project.html#generating-data",
    "title": "1 Introduction",
    "section": "4.1 Generating Data",
    "text": "4.1 Generating Data\nFor generating data we used 10 observable variables(p) \\(X_1,.....X_{10}\\). Number of variables are used as standard as per given in the paper procedures and we simulate 100 observation for each variable. For the next part we define 3 hidden variables - latent factors based on authors paper which imples the factors that influence observed variables.\n\\[\nV_1 \\sim \\mathcal{N}(0, 290)\n\\]\n\\[\nV_2 \\sim \\mathcal{N}(0, 300)\n\\]\nThe variables \\(V_1\\), \\(V_2\\) and \\(V_3\\) are 3 underlying hidden factors that govern th observed variables. The factors \\(V_1\\) & \\(V_2\\) are independent variables with high variances, whereas \\(V_3\\) is a linear combination of factors \\(V_1\\) & \\(V_2\\) and equation is given by:\n\\[\nV_3 = -0.3 V_1 + 0.925 V_2 + \\epsilon, \\quad \\text{where } \\epsilon \\sim \\mathcal{N}(0, 1)\n\\]\n\\[\nV_3 \\sim \\mathcal{N}(0, 283.8)\n\\]\nIt is given that \\(V_1\\) & \\(V_2\\) and \\(\\epsilon\\) are mutually independent of each other.\\(V_3\\) being the linear combination of \\(V_1\\) & \\(V_2\\) we find the mean and variance of \\(V_3\\). The calculation is provided in the Appendix. We constructed an exact covariance matrix of \\(X_1,.....,X_{10}\\) variables and each variables from \\(X_1\\) to \\(X_4\\) is constructed as:\n\\[\nX_i = V_1 + \\epsilon_i, \\quad \\text{where } \\epsilon_{ij} \\sim \\mathcal{N}(0, 1) \\quad \\text{for } i = 1, 2, 3, 4\n\\] \\[\nX_i = V_2 + \\epsilon_i, \\quad \\text{where } \\epsilon_{ij} \\sim \\mathcal{N}(0, 1) \\quad \\text{for } i = 5, 6, 7, 8\n\\] \\[\nX_i = V_3 + \\epsilon_i, \\quad \\text{where } \\epsilon_{ij} \\sim \\mathcal{N}(0, 1) \\quad \\text{for } i = 9, 10\n\\]\nwhere \\(\\epsilon_{ij}\\) is an independent Gaussian noise and \\(j \\in \\{1, 2, 3\\}\\) that indicates which latent factor group the variable belongs to. So, similarly for variables from \\(X_5\\) to \\(X_8\\) depend on \\(V_2\\) and variables \\(X_9\\) & \\(X_{10}\\) depends on \\(V_3\\). In our simulation, we get n = 100 observations for each of the 10 variables, resulting in a data matrix of size 10 × 100.\nBefore applying dimension reduction methods, on our simulated data matrix X, we standardized it so that each variable had mean zero and unit variance using scale which is a generic function whose default method centers the columns of a matrix. Standardization ensures that all variables contribute equally when performing principal component analysis (PCA) or Sparse PCA. Note that we do not proceed with SCoTLASS or Simple Thresholding method under simulation as our key interest only remains in PCA and SPCA method."
  },
  {
    "objectID": "Project 7 ML/Simulation spca STA315 Project.html#principal-component-analysis",
    "href": "Project 7 ML/Simulation spca STA315 Project.html#principal-component-analysis",
    "title": "1 Introduction",
    "section": "4.2 Principal Component Analysis",
    "text": "4.2 Principal Component Analysis\nWe first applied Principal Component Analysis using the ‘prcomp()’ function in R on our standardized dataset. It computes the principal components from the data matrix using Singular Value Decomposition (SVD) method as discussed in above methodology. This is generally the preferred method for numerical accuracy for obtaining stable principal components. We extracted the loading for the first three principal components, and we computed the proportion of variance explained by the first, second and third PCs using the standard PCA output. These loading and variances summarize how much of the variability in the data is captured by the principal components from Table 1."
  },
  {
    "objectID": "Project 7 ML/Simulation spca STA315 Project.html#sparse-principal-component-analysis",
    "href": "Project 7 ML/Simulation spca STA315 Project.html#sparse-principal-component-analysis",
    "title": "1 Introduction",
    "section": "4.3 Sparse Principal Component Analysis",
    "text": "4.3 Sparse Principal Component Analysis\nMoving on from Principal component analysis, we apply Sparse PCA using spca() function from elasticnet package. The formula takes the input of standardized data matrix, with K = 2 that computes first 2 components i.e. PC1 and PC2, we impose sparsity by specifying that each component should have exactly four non-zero loading (sparse = “varnum”, para = c(4,4)) in the simulation design with lambda set to standard default value of 1e-6.\n\n\nYou may wish to restart and use a more efficient way \nlet the argument x be the sample covariance/correlation matrix and set type=Gram \n\n\n\n\nTable 1: The results from PCA and SPCA Loadings\n\n\n\n\n\n\nVariable\nPCA PC1\nPCA PC2\nPCA PC3\nSPCA PC1\nSPCA PC2\n\n\n\n\nX1\n0.118\n-0.478\n0.085\n0.0\n-0.5\n\n\nX2\n0.118\n-0.478\n0.092\n0.0\n-0.5\n\n\nX3\n0.117\n-0.478\n0.095\n0.0\n-0.5\n\n\nX4\n0.118\n-0.478\n0.078\n0.0\n-0.5\n\n\nX5\n-0.391\n-0.146\n-0.266\n-0.5\n0.0\n\n\nX6\n-0.391\n-0.146\n-0.263\n-0.5\n0.0\n\n\nX7\n-0.391\n-0.146\n-0.274\n-0.5\n0.0\n\n\nX8\n-0.391\n-0.147\n-0.298\n-0.4\n0.0\n\n\nX9\n-0.409\n0.005\n0.573\n0.0\n0.0\n\n\nX10\n-0.409\n0.005\n0.580\n0.0\n0.0\n\n\nExplained Variance (%)\n59.700\n40.000\n0.080\n39.6\n39.8\n\n\n\n\n\n\n\n\nLater on, we obtain percentage of variance explained by each principal component that is obtained using the summary() function on the PCA object and it is extracted for PC1, PC2 & PC3. Similarly we compute percentage of variance explained for SPCA; given by pev output from spca function. The result are presented in Table 1 with first 10 rows showing the PCA and SPCA loadings for each variable. The last row of table summarizes the percentage of explained variances for the first three principal components under PCA method and the first two sparse principal components under SPCA method."
  },
  {
    "objectID": "Project 7 ML/Simulation spca STA315 Project.html#results",
    "href": "Project 7 ML/Simulation spca STA315 Project.html#results",
    "title": "1 Introduction",
    "section": "4.4 Results",
    "text": "4.4 Results\nThe simulation result align closely with the results from thr paper presented by authors Hui Zou, Trevor Hastie, and Robert Tibshirani (2006). The result from first to PCs from PCA method catpture maximum total variance of 99.7% (PC1 explains 59.7% and PC2 explains 40%) which ideally shed the light upon the fact the simulated dataset is dominated by variables \\(V_1\\) and \\(V_2\\). Also, the loading values in Table 1 all behave similarly with Synthetic Example. The magnitude showcases that PC1 captures variability related to latent factors \\(V_2\\) using \\(X_5, X_6, X_7, X_8\\) variables as they have large loading values of -0.391. On the other hand, variables \\(X_1, X_2, X_3, X_4\\) show loadings of -0.478 on PC2 corresponding to latent factor \\(V_1\\).\nFor SPCA loading values observe from Table 1 that the method selects only important variables and forces other variable to be set to zero. Hence in the simulation we produce ideal sparse PCs. Note we did not use adjusted variance. The SPCA loadings match th simulation setup almost exactly The magnitude showcases that PC1 captures variability related to latent factor V2 using \\(X_5, X_6, X_7, X_8\\) variables as they have large loading values of -0.5 On the other hand, variables \\(X_1, X_2, X_3, X_4\\) show strong loadings of -0.5 on PC2 corresponding to latent factor \\(V_1\\). To conclude with SPCA primarily focus on most important variables while still explaining high variance of 79.4% and also notice it is easy to interpret the key variables being highlighted as important."
  },
  {
    "objectID": "Project 8/Levy_Processes.html#properties-of-lévy-processes",
    "href": "Project 8/Levy_Processes.html#properties-of-lévy-processes",
    "title": "Stochastic Processes: Exploring Lévy Processes\nSTA348H5F: Introduction to Stochastic Processes\nDr. Omidali A. Jazi",
    "section": "Properties of Lévy Processes",
    "text": "Properties of Lévy Processes\nLet ( X ) be a random variable where ( X ) represents the state of a process at time ( t ), where ( t ). A stochastic process ( X = { X_t : t } ) is said to be a Lévy Process if the following conditions hold:\n1. Initial start of the process:\nThe process starts at zero in space ( R^d ), where (X_0 = 0), implying process starts at origin.\n2. Time-Homogeneity of increments\nThe state X has stationary increments ,that is, increments depends on the lenght of time interval represented by ‘s’, [ X_{t+s} - X_t ] Thus, increments depends only on s and is independent of time ‘t’. The statistical properties of the process’s assumes the increments are consistent over time.\n3. Continuity in Probability\nThe stochastic continuity ensures the process behaves predictably over small intervals, however there may be random change at larger intervals\nFor any ( &gt; 0 ): [ (|X_{t+s} - X_t| &gt; ) s . ]\n( ) evaluates to the margin of error, the probability of increment being larger than margin error implies the Levy process behaves predictability in small steps.\n4. Property of Independent Increments\nThe independence increment assumption ensures that the sequence interval do not overlap.\n[ s_1 t_1 s_2 t_2 s_n t_n. ]\nThe increment is the change in process denoted by over the respective given time interval ([s_i, t_i])\n[ X_{t_i} - X_{s_i}. ]\nLevy processes possesses similarities to Markov chain from the properties view; nevertheless, on grounds for continuity in probability, they are strong Markov chains. This report reviews the Levy processes from both the theoretical and the application perspectives."
  },
  {
    "objectID": "Project 8/Levy_Processes.html#infinitely-divisiblity",
    "href": "Project 8/Levy_Processes.html#infinitely-divisiblity",
    "title": "Stochastic Processes: Exploring Lévy Processes\nSTA348H5F: Introduction to Stochastic Processes\nDr. Omidali A. Jazi",
    "section": "Infinitely Divisiblity",
    "text": "Infinitely Divisiblity\nA random variable is infinitely divisible when it is the sum of any samller independent random variables that have all same type of distribution. Consider an a random variable X an event over time, imagine if it can be split into n smaller independent pieces or variables, with each piece distributed identically. The sum of identically distributed independent variables adds up to the random variable X.\n[ X = _{k=1}^n Y_k, Y_k ]\nHere,\n\n( X ): The original random variable.\n( Y_k ): The smaller, independent, identically distributed random variables that add up to ( X ).\n\nUsing the stationanry increment property, the distribution of ( X_t ), depends only on t:\n[ {X_t}(t) = ({X}(t))^t = (e{(t)})t = e^{t (t)} ]\nThe function ( (t) ) is called characteristics exponent, its purpose is to capture all the dynamics of Levy process. The three components of the charaterstics exponent are:\n\nDiffusion Component smoothed the continuous fluctuation caused by randomness, where mean of process at time (t ) is \\(at\\) and \\(\\sigma^2 t\\) is\n\n[ X_t N(at, ^2 t), ]\nThe characteristics function of a normal random variable, the formula we get:\n[ _{X_t}(t) = (- ^2 t^2), ]\nThis shows explain continous random changes.\n\nDrift represents the deterministic, linear trend in the process. It account for average constant rate at whihc process increases or decreases over time\n\n[ _{X_t}(t) = (i a t), ]\nThe drift term, models predictable steady changes in the process, such as a stock price that consistently grows by a fixed amount each year.\n\nJump Component The jump component is discontinuous changes or “jumps” in the process. The jump component can be derived as jump occurs at times (S_n), where (X_t) increase by 1. The Levy measure ( (dx) = (dx) ) which quantifies rate and size of jumps in process.The jump term, models sudden & unpredictable changes, such as market crashes, abrupt system failures, or genetic mutations. The characteristic exponent for jump process, is denoted by ( (t) ).\n\n[ {} (e^{itx} - 1 - itx {|x| &lt; 1}) (dx), ]\nTo provide an example from physics, the Brownian motion is one of the simplest stochastic processes, known as Lévy processes. Composed solely by purely small random changes in motion, employed particle moves are characterized by the diffusion term, while their variance \\(\\sigma^2\\) defines continuous variations. It is a measure of the diffusion term left by the Lévy-Khintchine formula of stochastic process characteristic function. Poisson process another basic example of Lévy process which is defined by its pure jump nature. The jump measure ( (dx) ) determines the rate of jumps, which is actually, a single event. However, the Poisson process has jumps which makes it to be one of the most useful for modeling events which are rare or at least abrupt like system’s failures in a process."
  },
  {
    "objectID": "Project 9/Data Analysis_v1.html#airline-dataset",
    "href": "Project 9/Data Analysis_v1.html#airline-dataset",
    "title": "Regression Analysis: Technical Report",
    "section": "2.1 Airline Dataset",
    "text": "2.1 Airline Dataset\n\n2.1.1 Dataset Source\nThe data used in this paper were collected from Kaggle website Library. The specific dataset used in this paper is the ‘Airline Customer satisfaction’ (Osuyah 2023). The dataset is categorized by collecting feedback from customer after they have traveled with the airline and determining if customer was satisfied or dissatisfied by the services. The actual name of the airline is not provided in the data due privacy concerns so the alias name used is Invistico Airlines.\nKey variables\nThe dataset provides a comprehensive overview of various factors influencing customer satisfaction in airline services. It includes a variety of metrics categorized into key domains, such as demographic details, travel characteristics, service quality ratings, and performance metrics. We used many variables such as Demographic Information: gender, passenger age, Travel Characteristics: class: (Economy, Business, or Economy Plus), flight distance, Service Quality Metrics (Rated on a scale of 0–5): seat_comfort, food and drink, gate location, inflight entertainment, ease of online booking, cleanliness and other services like leg room, check-in service. One other parameter was departure delay in minutes that is delay in flight departure time"
  },
  {
    "objectID": "Project 9/Data Analysis_v1.html#youtube-dataset",
    "href": "Project 9/Data Analysis_v1.html#youtube-dataset",
    "title": "Regression Analysis: Technical Report",
    "section": "2.2 Youtube Dataset",
    "text": "2.2 Youtube Dataset\n\n2.2.1 Dataset Source\nThe data used in this paper were collected from Kaggle website Library. The dataset used in this paper is the ‘Youtube Channel Real Performance Analytics’ (Alexey 2023). The YouTube video analytics, focuses on key performance, engagement, revenue, and audience metrics. It enables insights into video duration, upload timing, and ad impressions’ impact on monetization and audience retention.\nKey variables\nThis dataset provides comprehensive information about video performance, revenue generation, and audience engagement, which can be used for detailed analytics and insights and paper focuses on predicting the estimated revenue (USD) as the dependent variable and three independent variables, the number of views on the playlist associated with the videos (playlist_views), the number of ad impressions generated by the videos (ad_impressions), and the duration of the video content (video_duration).\nData used in this paper was downloaded, cleaned and analyzed with the programming language R (R Core Team 2022). Also the paper used with support of additional packages in R: ‘tidyverse’ (Wickham et al. 2019), ‘janitor’ (Firke 2021), ‘dplyr’ (Wickham et al. 2022), ‘readr’ (Wickham, Hester, and Bryan 2024), ‘ggplot’ (Wickham 2016), ‘AER’(Kleiber and Zeileis 2008), ‘gtsummary’ (Sjoberg et al. 2021), ‘reshape2’ (Wickham 2007), ‘broom’ (Robinson, Hayes, and Couch 2024), ‘MASS’ (Venables and Ripley 2002)."
  },
  {
    "objectID": "Project 9/Data Analysis_v1.html#binomial-logisitic-regression",
    "href": "Project 9/Data Analysis_v1.html#binomial-logisitic-regression",
    "title": "Regression Analysis: Technical Report",
    "section": "3.1 Binomial Logisitic Regression",
    "text": "3.1 Binomial Logisitic Regression\nThe Invisco airline collected the feedback from customers. The model aims to find the probability of the binary outcome i.e. passenger satisfaction:\n\n( y = 0 ): Customer is dissatisfied.\n( y = 1 ): Customer is satisfied.\n\nThe binomial logistic regression model predicts the likelihood of being satisfied (P(Y = 1)) based on independent variables. The probability of a binary outcome Y being 1 is modeled as:\n\\[\nP(Y = 1) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p}}\n\\]\nOr equivalently, the log-odds form is given by:\n\\[\n\\log\\left(\\frac{P(Y=1)}{1 - P(Y=1)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\nWhere:\n\n\\(P(Y = 1)\\): Probability that the customer is satisfied.\n\\(\\frac{P(Y=1)}{1 - P(Y=1)}\\): Odds of satisfaction.\n\\(\\beta_0\\): Intercept of the model.\n\\(\\beta_1, \\beta_2, \\dots, \\beta_p\\): Coefficients representing the effect of each predictor"
  },
  {
    "objectID": "Project 9/Data Analysis_v1.html#multiple-linear-regression",
    "href": "Project 9/Data Analysis_v1.html#multiple-linear-regression",
    "title": "Regression Analysis: Technical Report",
    "section": "3.2 Multiple Linear Regression",
    "text": "3.2 Multiple Linear Regression\nUsing a Multiple Linear Regression (MLR) model to predict the dependent variable Estimated Revenue (USD) based on a set of independent variables from the YouTube dataset. The goal is to understand how key performance metrics influence revenue generation and provide actionable insights for optimization.\nThe general equation for Multiple Linear Regression is: \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon\n\\]\nFor this analysis, the log-transformed equation used:\n\\[\n\\log(\\text{Estimated Revenue (USD)}) = \\beta_0 + \\beta_1 (\\text{Playlist Views}) + \\beta_2 (\\text{Ad Impressions}) + \\beta_3 (\\text{Video Duration}) + \\epsilon\n\\]"
  },
  {
    "objectID": "Project 9/Data Analysis_v1.html#binomial-logisitic-regression-1",
    "href": "Project 9/Data Analysis_v1.html#binomial-logisitic-regression-1",
    "title": "Regression Analysis: Technical Report",
    "section": "4.1 Binomial Logisitic Regression",
    "text": "4.1 Binomial Logisitic Regression\nThe correlation matrix from Figure 1 highlights strong positive correlation between arrival delays and departure delays of 0.97, indicating that delays at departure often propagate to arrival times. Additionally, service metrics such as ease of online booking and online boarding are moderately correlated (0.68), reflecting their positive relationship with customer experience. Service-related variables like inflight entertainment and food and drink also show moderate positive associations (~0.43), suggesting that satisfaction in one area influences the other. This analysis underscores the need to address extreme delays while enhancing clustered service areas for better customer satisfaction.\n\n\n\n\n\n\n\n\nFigure 1: Heat Map or corr matrix\n\n\n\n\n\nThe Table 1 shows the result of binomial logistic regression analysis. The models uses multiple predictors. The predictors include numerical predictors and categorical predictors, so we use Gender and female as the reference level and for Business class as the reference level. In the Table 1 we observe, male passenger report higher satisfaction than females, indicating they are more than twice (2.43) likely to be satisfied. Similarly, customers in Eco and Eco Plus classes have a higher relative satisfaction likelihood after adjusting for expectations and service quality using business class as the reference level. However, services offered by the airline such as inflight entertainment, seat comfort, cleanliness, food & drink and ease of online booking negatively correlated by customer satisfaction. The Inflight entertainment is one of the strongest negative predictors of customer satisfaction as Inflight Entertainment (-0.8165) the largest negative log-odds, indicating that poor ratings in entertainment strongly reduces satisfaction.\n\n\n\n\nTable 1: Model 1\n\n\n\n\nBinomial Logistic Regression Results\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nodds_ratio\n\n\n\n\n(Intercept)\n5.3191244\n0.0559170\n95.125310\n0\n204.2050029\n\n\nflight_distance\n0.0000719\n0.0000081\n8.855758\n0\n1.0000719\n\n\ndeparture_delay_in_minutes\n0.0046094\n0.0002130\n21.643370\n0\n1.0046200\n\n\ncleanliness\n-0.1034490\n0.0075648\n-13.675118\n0\n0.9017220\n\n\ninflight_entertainment\n-0.8164875\n0.0074769\n-109.202016\n0\n0.4419814\n\n\nseat_comfort\n-0.2606842\n0.0087304\n-29.859229\n0\n0.7705242\n\n\nage\n-0.0043520\n0.0005263\n-8.268296\n0\n0.9956575\n\n\ngenderMale\n0.8899748\n0.0155075\n57.389899\n0\n2.4350682\n\n\nclassEco\n1.1505727\n0.0166927\n68.926771\n0\n3.1600022\n\n\nclassEco Plus\n0.9922172\n0.0302824\n32.765479\n0\n2.6972081\n\n\nfood_and_drink\n0.3244823\n0.0087978\n36.882340\n0\n1.3833143\n\n\nleg_room_service\n-0.2757300\n0.0065272\n-42.243466\n0\n0.7590178\n\n\ngate_location\n-0.0576163\n0.0069299\n-8.314168\n0\n0.9440121\n\n\nease_of_online_booking\n-0.4804120\n0.0068167\n-70.475289\n0\n0.6185285\n\n\ncheckin_service\n-0.2724898\n0.0062839\n-43.363294\n0\n0.7614812\n\n\n\n\n\n\n\n\nMoving from logistic regression we perform model comparison to ensure both model are accurate. A full model includes chosen predictors whereas the reduced model includes a subset of predictor use in full model as seen in Table 1. The reduced logistic regression model in Table 2 shows the relationship between customer satisfaction and a selected set of predictors. The compare the models we use AIC as statistical measure used to evaluate and compare the quality of models.\n\n\n\n\nTable 2: Model 2\n\n\n\n\nReduced Binomial Logistic Regression Model Results\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nStd. Error\nz-value\np-value\nConf. Low\nConf. High\n\n\n\n\n(Intercept)\n3.7406\n0.0417\n89.6859\n0\n3.6590\n3.8225\n\n\ndeparture_delay_in_minutes\n0.0041\n0.0002\n20.3415\n0\n0.0037\n0.0045\n\n\ncleanliness\n-0.4586\n0.0064\n-71.8510\n0\n-0.4711\n-0.4461\n\n\ninflight_entertainment\n-0.9084\n0.0069\n-132.5327\n0\n-0.9219\n-0.8950\n\n\nseat_comfort\n-0.3667\n0.0081\n-45.3466\n0\n-0.3826\n-0.3509\n\n\nage\n-0.0088\n0.0005\n-18.3363\n0\n-0.0097\n-0.0078\n\n\ngenderMale\n0.9216\n0.0146\n63.1258\n0\n0.8930\n0.9502\n\n\nclassEco\n1.1685\n0.0155\n75.1639\n0\n1.1381\n1.1990\n\n\nclassEco Plus\n1.0175\n0.0286\n35.5922\n0\n0.9615\n1.0736\n\n\nfood_and_drink\n0.3745\n0.0076\n49.2733\n0\n0.3597\n0.3895\n\n\n\n\n\n\n\n\nIt helps to identify the model that best balances goodness-of-fit and simplicity. From Table 3 we obverse AIC for 2 models. The lower the AIC better the model, therefore, Table 1 AIC equivalent to 108635.4 results are better compared to reduced model AIC. Table 3 shows the full model explains customer satisfaction substantially better than the reduced model, as evidenced by the large reduction in deviance that indicates additional predictors in the full model provide a significantly better fit to the data. We observe p-value is extremely small so we reject the null hypothesis.\n\n\n\n\nTable 3: Analysis of Deviance Table: Comparison of Reduced and Full Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nResidual Df\nResidual Deviance\nDf\nDeviance\nPr(&gt;Chi)\n\n\n\n\nReduced Model\n129477\n118909.3\nNA\nNA\nNA\n\n\nFull Model\n129472\n108605.4\n5\n10303.97\n0"
  },
  {
    "objectID": "Project 9/Data Analysis_v1.html#multiple-linear-regression-1",
    "href": "Project 9/Data Analysis_v1.html#multiple-linear-regression-1",
    "title": "Regression Analysis: Technical Report",
    "section": "4.2 Multiple Linear Regression",
    "text": "4.2 Multiple Linear Regression\nFor the youtube datatset which analysis evaluates the performance of a regression model. We compare multiple linear regression with a log transformed regression model. From Table 4 the model evaluates estimated revenue generated in USD by YouTube videos. The intercept (0.514) is statistically significant, indicating when all predictors are zero log-transformed revenue is approximately 0.5134 units. All predictors are statistically significant and they have all have positive relationship with the dependent variables. The p-value is less than 0.05, indicating strong evidence. Ad impressions are the most impact predictor, as 1-unit increase in ad impressions leads to a 0.003% increase in revenue.\n\n\n\n\nTable 4: Regression Analysis of Log-Transformed Estimated Revenue\n\n\n\n\n\n\nTerm\nEstimate\nStd. Error\nt-value\np-value\n\n\n\n\n(Intercept)\n0.51378\n0.09705\n5.29412\n0.00000\n\n\nplaylist_views\n0.00026\n0.00008\n3.42790\n0.00068\n\n\nad_impressions\n0.00003\n0.00000\n20.09616\n0.00000\n\n\nvideo_duration\n0.00070\n0.00012\n5.61654\n0.00000\n\n\n\n\n\n\n\n\nFigure 2 shows QQ-Plot follows a theoretical distribution indicating the residuals are normally distributed which help to check the linearity assumptions. As the residuals re distributed around red line with some curvature suggesting slight deviation. We can concludethe residuals are near normal.\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nIn Figure 3 we plot the relationship between residuals and predicted values for a log transformed regression. The The residuals are centered around the red horizontal line at y=0, indicating that the model does not exhibit systematic bias in predictions (e.g., overestimating or underestimating consistently). Ideally, the residuals should have a constant spread (homoscedasticity). There appears to be heteroscedasticity, as the spread of residuals increases with larger fitted values. This suggests that the variance of residuals is not constant and increases as fitted values grow. A non-linear relationship between the predictors and the response variable that is not captured by the model.\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nWe considered many models including by introducing the polynomial or interaction terms in the model to capture non-linear relationships ,however, we did not see an improvement. We added complexity but it did not translate into better predictive power, and the model’s assumptions were still not fully satisfied. Adding more complexity to the model may lead to over fitting, especially if the added terms are not improving the model’s predictive accuracy."
  }
]